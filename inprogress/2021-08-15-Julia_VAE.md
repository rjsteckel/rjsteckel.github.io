@def title = "VAE Anomaly Detection"

I haven't written a post in quite a while. During that time, Julia has gotten a lot of my interest. Also during that time, Variational Auto Encoders have caught my eye. I find the combination of deep learning with traditional probability distributions pretty cool. So, it seemed like it's a good time to write about both. 

First, a basic description of an autoencoder. An autoencoder combines two neural networks, an encoder and a decoder. An input vector is given to the encoder which typically reduces the vector down to a lower dimensional vector. Similar to PCA, this lower dimensional vector should capture certain redundant information between variables in the input vector. That lower-dimensional vector is then given an input to the decoder. The decoder transforms the vector back to the original input vector. These transformations are not perfect. Some flavor of gradient descent is used to train the weights of the encoder and decoder to minimize the loss between the input vector and the transformed output vector. 

The basic idea with VAEs is to use an encoder and decoder, but instead of a single, lower dimensional vector that represents information about the input vector, the vector(s) represents parameters of a probability distribution. One option is to have your encoder output two vectors which represent a vector of means and a vector of variances. 

This is what we'll do. 

First, we'll import the packages we're going to need.

```julia
using Flux
using Flux: train!, params, throttle, @functor, @nograd, @epochs
using Flux: Data.DataLoader, Losses
using Distributions
using RDatasets
```

Next we'll load up some data. We'll use arguably the most famous data set of all, the iris data set. Luckily, Julia has a package containing many sample datasets, so it's easy.


```julia
iris = dataset("datasets", "iris");

select!(iris, Not(:Species));
X = transpose(convert(Matrix, iris));

# Center and scale
X = (X .- mean(X, dims=2)) ./ std(X, dims=2)

# create a Flux data loader
loader = DataLoader(X, batchsize=4, shuffle=true);
```

Now the encoder is a function that maps the input to parameters of a Normal distribution. This encoder (and correspoding decoder) are neural nets, but not deep. This is just a simple VAE example. However, these neural nets could get quite interesting. You might use sequences with LSTMs or convolutions.

Model layout

```julia
# computes p(z|x)
struct Encoder
    linear1
    μ
    logσ
end
@functor Encoder

Encoder(input_dim::Int, latent_dim::Int, hidden_dim1::Int) = Encoder(
    Dense(input_dim, hidden_dim1, relu),   # linear1    
    Dense(hidden_dim1, latent_dim),        # μ
    Dense(hidden_dim1, latent_dim),        # logσ
)
function (encoder::Encoder)(x)    
    h1 = encoder.linear1(x)
    encoder.μ(h1), encoder.logσ(h1)
end

# Create a decoder
# input: z
# computes p(x|z)
Decoder(input_dim::Int, latent_dim::Int, hidden_dim1::Int) = Chain(
    Dense(latent_dim, hidden_dim1, relu),   
    Dense(hidden_dim1, input_dim)
)
```

Next, a key piece of the model, the reparameterization.

```julia
function reparameterize(μ, logvar)
    ϵ = randn(Float32, size(logvar))
    σ² = exp.(logvar)
    z = μ + ϵ .* sqrt.(σ²)
    z, μ, logvar
end
```


```julia
# negative log-likelihood (with regularization)
function loss(x; β=0.1)    
    x̂, μ, logσ = model(x)

    # make output like input (reconstruction error)
    reconstruction_loss = Losses.mse(x̂, x)

    # make it the shape of a gaussian  (KL-divergence)
    # q = MvNormal(μ, exp.(logσ))    
    # p = MvNormal(repeat([0.], 2), repeat([1.], 2))    # MvNorma(0, I)
    # kl_q_p = KullbackLeibler(q, p)
    
    #https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions
    len = size(x)[end]
    kl_q_p = 0.5f0 * sum(@. (exp(2f0 * logσ) + μ^2 -1f0 - 2f0 * logσ)) / len

    reconstruction_loss + (β * kl_q_p)
end

```

Next, we train this thing.

```julia
encoder = Encoder(4, 3, 2)
decoder = Decoder(4, 3, 2)

model = Chain(x -> encoder(x),
               x -> reparameterize(x[1], x[2]),
               x -> (decoder(x[1]),  x[2], x[3]))

ps = params(encoder, decoder);
opt = ADAM(0.001);

loss_vector = Vector{Float32}()
callback() = push!(loss_vector, loss(X))

@epochs 500 train!(loss, ps, loader, opt, cb=callback)
```

Interpreting this gets tricky. But, for a simple anomaly detector, we could train many examples of legitimate vectors. The weights will be set to values that best approximate the input vectors (given this network structure). For new data, we could run the vector through the encoder and generate two parameter vectors $[ \mu_1, \mu_2, ... ], [ \sigma_1, \sigma_2, ... ]$

Then we just use a pdf of $ N([ \mu_1, \mu_2, ... ], [ \sigma_1, \sigma_2, ... ]) $ to get a probability. Very low probabilities are considered anomomalous. 